{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/miniconda3/envs/keytotext/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from keytotext import pipeline\n",
    "import keybert\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import typing\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset corpus used is based on [Wikipedia business corpus](https://gricad-gitlab.univ-grenoble-alpes.fr/getalp/wikipediacompanycorpus/-/tree/master/). There are two approaches used here: either use `KeyBert` to generate keywords of the abstract text, or use the dataset's `infobox` to generate the keywords.\n",
    "\n",
    "This notebook assumes the wikipedia business corpus has been saved into `data/wikiepedia_companies`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KeyBert to generate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 2.18MB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 135kB/s]\n",
      "Downloading: 100%|██████████| 10.6k/10.6k [00:00<00:00, 10.7MB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 287kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 53.4kB/s]\n",
      "Downloading: 100%|██████████| 39.3k/39.3k [00:00<00:00, 110kB/s] \n",
      "Downloading: 100%|██████████| 438M/438M [00:33<00:00, 13.2MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 64.9kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 130kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:02<00:00, 161kB/s]  \n",
      "Downloading: 100%|██████████| 363/363 [00:00<00:00, 193kB/s]\n",
      "Downloading: 100%|██████████| 13.1k/13.1k [00:00<00:00, 4.67MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:05<00:00, 41.0kB/s] \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 122kB/s]\n"
     ]
    }
   ],
   "source": [
    "kw_model = keybert.KeyBERT(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wikipedia_companies/train_abstract.txt', 'r') as f:\n",
    "    train = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmc connect burson-marsteller is a premier perception management firm that provides communication solutions . the company was founded in 1995 by yomi badejo okusanya in ikeja , lagos , nigeria . cmc connect have exclusive affiliation with burson-marsteller a leading global public relations firm and became cmc connect burson-marsteller in 2015 .\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tompkins financial corporation is small diversified financial <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">services</span> company based in ithaca new <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">york</span> it is the \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">parent</span> of the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">tompkins trust</span> as well as several other banks an insurance agency and wealth management division the \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">bank traces</span> history to 1836 when the tompkins county bank was chartered by special act of the new <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">york</span> state \n",
       "assembly after the national bank act of 1864 it was reorganized into the tompkins county national bank\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tompkins financial corporation is small diversified financial \u001b[30;48;2;255;255;0mservices\u001b[0m company based in ithaca new \u001b[30;48;2;255;255;0myork\u001b[0m it is the \n",
       "\u001b[30;48;2;255;255;0mparent\u001b[0m of the \u001b[30;48;2;255;255;0mtompkins trust\u001b[0m as well as several other banks an insurance agency and wealth management division the \n",
       "\u001b[30;48;2;255;255;0mbank traces\u001b[0m history to 1836 when the tompkins county bank was chartered by special act of the new \u001b[30;48;2;255;255;0myork\u001b[0m state \n",
       "assembly after the national bank act of 1864 it was reorganized into the tompkins county national bank\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tompkins <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">financial corporation</span> small diversified financial services company based in ithaca new <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">york</span> it is the \n",
       "parent of the tompkins trust company as well as several other banks an insurance agency and wealth management \n",
       "division the bank traces its history to 1836 when the tompkins county bank was chartered by special act of the new \n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">york</span> state assembly after the national bank act of 1864 it was reorganized into the tompkins county national bank\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tompkins \u001b[30;48;2;255;255;0mfinancial corporation\u001b[0m small diversified financial services company based in ithaca new \u001b[30;48;2;255;255;0myork\u001b[0m it is the \n",
       "parent of the tompkins trust company as well as several other banks an insurance agency and wealth management \n",
       "division the bank traces its history to 1836 when the tompkins county bank was chartered by special act of the new \n",
       "\u001b[30;48;2;255;255;0myork\u001b[0m state assembly after the national bank act of 1864 it was reorganized into the tompkins county national bank\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 150\n",
    "n = 8\n",
    "n_gram = (1,2)\n",
    "\n",
    "with_mmr = kw_model.extract_keywords(\n",
    "    train[i], \n",
    "    keyphrase_ngram_range=n_gram, \n",
    "    top_n=8, \n",
    "    stop_words=\"english\", \n",
    "    highlight=True,\n",
    "    use_mmr=True,\n",
    "    diversity=0.7\n",
    ")\n",
    "\n",
    "with_maxsum = kw_model.extract_keywords(\n",
    "    train[i], \n",
    "    keyphrase_ngram_range=n_gram, \n",
    "    top_n=5, \n",
    "    stop_words=\"english\", \n",
    "    highlight=True,\n",
    "    use_maxsum=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('tompkins trust', 0.7036),\n",
       "  ('york', 0.3764),\n",
       "  ('bank traces', 0.3091),\n",
       "  ('1864 reorganized', 0.1689),\n",
       "  ('state', 0.1163),\n",
       "  ('services', 0.1083),\n",
       "  ('parent', 0.0809),\n",
       "  ('act new', 0.0577)],\n",
       " [('banks insurance', 0.3737),\n",
       "  ('york', 0.3764),\n",
       "  ('based ithaca', 0.4014),\n",
       "  ('financial corporation', 0.4192),\n",
       "  ('parent tompkins', 0.48)])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_mmr, with_maxsum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate time taken to extract keywords from the dataset in order to determine the appropriate parameters to use for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def estimate_time_taken(params: typing.Dict) -> pd.Series:\n",
    "    num_rows = []\n",
    "    time_taken = []\n",
    "    for n in [5, 6, 7, 8]:\n",
    "        for i in [32, 64, 128, 256, 512]:\n",
    "            ns.append(n)\n",
    "            num_rows.append(i)\n",
    "            start = time.time()\n",
    "            kw_model.extract_keywords(\n",
    "                train[:i], \n",
    "                keyphrase_ngram_range=(1, 2), \n",
    "                top_n=n, \n",
    "                stop_words=\"english\", \n",
    "                highlight=True,\n",
    "                **params\n",
    "            )\n",
    "            time_taken.append(round(time.time() - start, 2))\n",
    "            print(f\"top_n = {n}, batch_size = {i}, took {time_taken[-1]}s\")\n",
    "    \n",
    "    # prepare multiindex\n",
    "    tuples = list(zip(ns, num_rows))\n",
    "    index = pd.MultiIndex.from_tuples(tuples, names=[\"top_n\", \"batch_size\"])\n",
    "    return pd.Series(time_taken, index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_n = 5, batch_size = 32, took 0.51s\n",
      "top_n = 5, batch_size = 64, took 0.82s\n",
      "top_n = 5, batch_size = 128, took 1.89s\n",
      "top_n = 5, batch_size = 256, took 2.23s\n",
      "top_n = 5, batch_size = 512, took 4.86s\n",
      "top_n = 6, batch_size = 32, took 0.34s\n",
      "top_n = 6, batch_size = 64, took 0.83s\n",
      "top_n = 6, batch_size = 128, took 1.43s\n",
      "top_n = 6, batch_size = 256, took 1.98s\n",
      "top_n = 6, batch_size = 512, took 4.85s\n",
      "top_n = 7, batch_size = 32, took 0.35s\n",
      "top_n = 7, batch_size = 64, took 1.19s\n",
      "top_n = 7, batch_size = 128, took 1.2s\n",
      "top_n = 7, batch_size = 256, took 1.94s\n",
      "top_n = 7, batch_size = 512, took 5.53s\n",
      "top_n = 8, batch_size = 32, took 0.39s\n",
      "top_n = 8, batch_size = 64, took 0.69s\n",
      "top_n = 8, batch_size = 128, took 1.1s\n",
      "top_n = 8, batch_size = 256, took 2.06s\n",
      "top_n = 8, batch_size = 512, took 5.28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top_n  batch_size\n",
       "5      32            0.51\n",
       "       64            0.82\n",
       "       128           1.89\n",
       "       256           2.23\n",
       "       512           4.86\n",
       "6      32            0.34\n",
       "       64            0.83\n",
       "       128           1.43\n",
       "       256           1.98\n",
       "       512           4.85\n",
       "7      32            0.35\n",
       "       64            1.19\n",
       "       128           1.20\n",
       "       256           1.94\n",
       "       512           5.53\n",
       "8      32            0.39\n",
       "       64            0.69\n",
       "       128           1.10\n",
       "       256           2.06\n",
       "       512           5.28\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_mmr = estimate_time_taken({\"use_mmr\": True, \"diversity\": 0.6})\n",
    "with_mmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_maxsum = estimate_time_taken({'use_maxsum': True, \"nr_candidates\": 5})\n",
    "with_maxsum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using infobox and some preprocessing\n",
    "We will use the infobox dataset information to extract the relevant keywords, whilst ignoring other properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wikipedia_companies/train_infobox.txt\", 'r') as f:\n",
    "    infobox = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name1[ ic group ], headquarters1[ copenhagen denmark ], founded1[ 2001 ], industry1[ fashion ], type1[ company ], key people1[ mads ryder (ceo) ], products1[ clothing ]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infobox[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exclude = ('found', 'key people', 'number', 'owner', 'defunct')\n",
    "\n",
    "def to_include(row: str):\n",
    "    return not row.strip().startswith(exclude)\n",
    "\n",
    "def extract(s: str):\n",
    "    # extracts the keywords between [ ]\n",
    "    # then strips the keywords of whitespaces\n",
    "    t = re.search(r'(?<=\\[).+?(?=\\])', s)\n",
    "    return s[t.start() + 1: t.end() - 1]\n",
    "\n",
    "def parse_string(s: str):\n",
    "    # 1. split each company (item within infobox) into a list of its properties\n",
    "    props = s.split(',')\n",
    "\n",
    "    # 2. remove certain properties\n",
    "    props = list(filter(to_include, props))\n",
    "\n",
    "    # 3. use regex to extrac the keyword from within each property\n",
    "    keywords = map(extract, props)\n",
    "    return list(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('name1[ bce inc. ], headquarters1[ canada ], founded1[ 1983 ], industry1[ telecommunications ], industry2[ mass media ], key people1[ george cope (ceo) ], products1[ fixed line and ], products2[ mobile telephony ], products3[ internet services ], founder1[ charles fleetford sise ]\\n',\n",
       " ['bce inc.',\n",
       "  'canada',\n",
       "  'telecommunications',\n",
       "  'mass media',\n",
       "  'fixed line and',\n",
       "  'mobile telephony',\n",
       "  'internet services'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infobox[3], parse_string(infobox[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_infobox(infobox: typing.List[str], batch_size: int = 512):\n",
    "    for i in range(0, len(infobox), batch_size):\n",
    "        yield list(map(parse_string, infobox[i: i + batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infobox_keywords = list(map(parse_string, infobox))\n",
    "len(infobox_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmc connect burson-marsteller',\n",
       " 'ikeja lagos lagos nigeria',\n",
       " 'media',\n",
       " 'perception management',\n",
       " 'public relations']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infobox_keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(keywords, texts, filename: str):\n",
    "    keywords_str = [\" \".join(keyword_list).replace(\",\", \"\") for keyword_list in keywords]\n",
    "    df = pd.DataFrame({\"keywords\": keywords_str, \"text\": texts})\n",
    "\n",
    "    dest_path = os.path.join('data', 'finetune', filename)\n",
    "    df.to_csv(dest_path, index=False)\n",
    "\n",
    "def make_datasets():\n",
    "    for dataset in ['train', 'dev', 'test']:\n",
    "        infobox_fn = f\"{dataset}_infobox.txt\"\n",
    "        abstract = f\"{dataset}_abstract.txt\"\n",
    "\n",
    "        infobox_path = os.path.join(\"data\", \"wikipedia_companies\", infobox_fn)\n",
    "        body_path = os.path.join(\"data\", \"wikipedia_companies\", abstract)\n",
    "\n",
    "        with open(infobox_path, 'r') as info_f:\n",
    "            infobox = info_f.readlines()\n",
    "        \n",
    "        with open(body_path, 'r') as body_f:\n",
    "            body = body_f.readlines()\n",
    "        \n",
    "        infobox_keywords = list(map(parse_string, infobox))\n",
    "        create_csv(infobox_keywords, body, f\"{dataset}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(directory_of_csvs: str):\n",
    "    import glob\n",
    "\n",
    "    csvs = glob.glob(f\"{directory_of_csvs}/*\")\n",
    "    for file in csvs:\n",
    "        df = pd.read_csv(file)\n",
    "        dataset = file.rsplit('/', 1)[-1].split('.')[0]\n",
    "\n",
    "        print(f\"{dataset} has {len(df)} rows.\")\n",
    "        print(f\"Empty or null values: {df.isnull().value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 35384 rows.\n",
      "Empty or null values: keywords  text \n",
      "False     False    35384\n",
      "dtype: int64\n",
      "test has 4368 rows.\n",
      "Empty or null values: keywords  text \n",
      "False     False    4368\n",
      "dtype: int64\n",
      "dev has 3929 rows.\n",
      "Empty or null values: keywords  text \n",
      "False     False    3929\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sanity_check(\"data/finetune/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new kolb aircraft aerospace company kit aircraft</td>\n",
       "      <td>the new kolb aircraft company is an american a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kum &amp; go retail convenience stores</td>\n",
       "      <td>kum may refer to : kum , a serbian and ukraini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>canadian tire financial services oakville fina...</td>\n",
       "      <td>canadian tire financial services ltd. , doing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bronner bros. marietta georgia private african...</td>\n",
       "      <td>the bronner bros. enterprise is one of the lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rgb entertainment argentina television product...</td>\n",
       "      <td>rgb entertainment is a production company from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords  \\\n",
       "0   new kolb aircraft aerospace company kit aircraft   \n",
       "1                 kum & go retail convenience stores   \n",
       "2  canadian tire financial services oakville fina...   \n",
       "3  bronner bros. marietta georgia private african...   \n",
       "4  rgb entertainment argentina television product...   \n",
       "\n",
       "                                                text  \n",
       "0  the new kolb aircraft company is an american a...  \n",
       "1  kum may refer to : kum , a serbian and ukraini...  \n",
       "2  canadian tire financial services ltd. , doing ...  \n",
       "3  the bronner bros. enterprise is one of the lar...  \n",
       "4  rgb entertainment is a production company from...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/finetune/test.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/finetune/train.csv\")\n",
    "dev_df = pd.read_csv(\"data/finetune/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keytotext import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mashrielbrian\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230121_140230-t5b3rrtt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ashrielbrian/keytotext/runs/t5b3rrtt\" target=\"_blank\">daily-shape-2</a></strong> to <a href=\"https://wandb.ai/ashrielbrian/keytotext\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ashrielbrian/keytotext\" target=\"_blank\">https://wandb.ai/ashrielbrian/keytotext</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ashrielbrian/keytotext/runs/t5b3rrtt\" target=\"_blank\">https://wandb.ai/ashrielbrian/keytotext/runs/t5b3rrtt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/miniconda3/envs/keytotext/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/miniconda3/envs/keytotext/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/miniconda3/envs/keytotext/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9829/9829 [38:59<00:00,  4.20it/s, loss=1.92, v_num=rrtt, train_loss=1.740, val_loss=2.010]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9829/9829 [39:02<00:00,  4.20it/s, loss=1.92, v_num=rrtt, train_loss=1.740, val_loss=2.010]\n"
     ]
    }
   ],
   "source": [
    "model = trainer()\n",
    "model.from_pretrained(\"t5-base\")\n",
    "model.train(\n",
    "    train_df, \n",
    "    dev_df, \n",
    "    batch_size=4, \n",
    "    max_epochs=5,\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/Documents/keytotext/model is already a clone of https://huggingface.co/ashrielbrian/t5-base-wikipedia-companies-keywords. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "# requires git-lfs\n",
    "# on ubuntu, to install git lfs:\n",
    "# 1. curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# 2. sudo apt-get install git-lfs\n",
    "# 3. git-lfs install \n",
    "\n",
    "token = \"\" # huggingface token\n",
    "model_repo = Repository(\n",
    "    \"model\",\n",
    "    \"ashrielbrian/t5-base-wikipedia-companies-keywords\",\n",
    "    token=token,\n",
    "    git_user=\"ashrielbrian\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_name = \"t5-base-wikipedia-companies-keywords\"\n",
    "readme_txt = f\"\"\"\n",
    "        ---\n",
    "            language: \"en\"\n",
    "            thumbnail: \"Keywords to Sentences\"\n",
    "            tags:\n",
    "            - keytotext\n",
    "            - k2t\n",
    "            - Keywords to Sentences\n",
    "\n",
    "            model-index:\n",
    "            - name: {model_name}\n",
    "            ---\n",
    "\n",
    "            Idea is to build a model which will take keywords as inputs and generate sentences as outputs.\n",
    "\n",
    "            Potential use case can include: \n",
    "            - Marketing \n",
    "            - Search Engine Optimization\n",
    "            - Topic generation etc.\n",
    "            - Fine tuning of topic modeling models \n",
    "        \"\"\".strip()\n",
    "\n",
    "(Path(model_repo.local_dir) / \"README.md\").write_text(readme_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload file pytorch_model.bin: 100%|█████████▉| 847M/850M [02:33<00:00, 6.43MB/s] remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "remote: ----------------------------------------------------------\u001b[0;33m        \n",
      "remote: Your push was accepted, but with warnings:        \n",
      "remote: - Warning: empty or missing yaml metadata in repo card        \n",
      "remote: help: https://huggingface.co/docs/hub/model-cards#model-card-metadata\u001b[0;32m        \n",
      "remote: ----------------------------------------------------------        \n",
      "remote: Please find the documentation at:        \n",
      "remote: https://huggingface.co/docs/hub/model-cards#model-card-metadata\u001b[0;0m        \n",
      "remote: ----------------------------------------------------------        \n",
      "To https://huggingface.co/ashrielbrian/t5-base-wikipedia-companies-keywords\n",
      "   e51b24c..2274232  main -> main\n",
      "\n",
      "Upload file pytorch_model.bin: 100%|██████████| 850M/850M [02:39<00:00, 5.59MB/s]\n",
      "Upload file spiece.model: 100%|██████████| 773k/773k [02:38<00:00, 4.78kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/ashrielbrian/t5-base-wikipedia-companies-keywords/commit/2274232cf9e44e4ed98304e4b0f80f7b84e07bc5'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_repo.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model(model_dir=\"model\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'band and roll is a german leather goods company founded in 1912 by gerald h. band. the company was originally known as band and roll, but changed its name to band and roll in 1989.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(keywords=[\"band and roll\", \"german\", \"leather goods\", \"iphone\", \"ipad\"], use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel_repo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrepo_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Clone from a remote. If the folder already exists, will try to clone the\n",
      "repository within it.\n",
      "\n",
      "If this folder is a git repository with linked history, will try to\n",
      "update the repository.\n",
      "\n",
      "Args:\n",
      "    repo_url (`str`):\n",
      "        The URL from which to clone the repository\n",
      "    token (`Union[str, bool]`, *optional*):\n",
      "        Whether to use the authentication token. It can be:\n",
      "         - a string which is the token itself\n",
      "         - `False`, which would not use the authentication token\n",
      "         - `True`, which would fetch the authentication token from the\n",
      "           local folder and use it (you should be logged in for this to\n",
      "           work).\n",
      "        - `None`, which would retrieve the value of\n",
      "          `self.huggingface_token`.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Raises the following error:\n",
      "\n",
      "    - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)\n",
      "      if the `token` cannot be identified and the `private` keyword is set to\n",
      "      `True`. The `token` must be passed in order to handle private repositories.\n",
      "\n",
      "    - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)\n",
      "      if an organization token (starts with \"api_org\") is passed. Use must use\n",
      "      your own personal access token (see https://hf.co/settings/tokens).\n",
      "\n",
      "    - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)\n",
      "      if you are trying to clone the repository in a non-empty folder, or if the\n",
      "      `git` operations raise errors.\n",
      "\n",
      "</Tip>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/keytotext/lib/python3.8/site-packages/huggingface_hub/repository.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "# To download and load the model from HF Hub\n",
    "model_repo.clone_from(f\"ashrielbrian/{model_name}\", token=token)\n",
    "loaded_model = trainer()\n",
    "loaded_model.load_model('model') # assuming `model_repo` is set to `model/` directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keytotext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a020000db87742df65c2cb0b70cb5199cfc634cff142c8dda32258e5336a4f49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
